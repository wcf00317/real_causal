import torch
from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR
from tqdm import tqdm
import os, logging
import numpy as np
from .evaluator import evaluate
from utils.general_utils import save_checkpoint


# ----------------------------
# utils
# ----------------------------
def _set_requires_grad(module, requires_grad: bool):
    if module is None:
        return
    for p in module.parameters():
        p.requires_grad = requires_grad


def _switch_stage_freeze(model, stage: int):
    """
    ÈíàÂØπÊñ∞Áâà CausalMTLModel ÁöÑÂÜªÁªìÁ≠ñÁï•„ÄÇ
    Model ÁªìÊûÑ:
      - encoder, projector_s, projector_p (Shared)
      - head_seg, head_depth, head_normal (Task Heads, depend on Z_s)
      - decoder_app, albedo_head, etc. (Recon/Decomp, depend on Z_p)
    """
    # Âü∫Á°ÄÊ£ÄÊü•ÔºåÈò≤Ê≠¢‰º†ÂÖ•‰∏çÂÖºÂÆπÁöÑÊ®°Âûã (Â¶Ç RawMTL)
    if not hasattr(model, 'projector_p'):
        return

    # === Stage 0: ÂàÜËß£È¢ÑÁÉ≠ (Decomposition Warmup) ===
    # ÁõÆÊ†áÔºöÂº∫Ëø´ Encoder Â≠¶‰π†Áâ©ÁêÜÂàÜËß£ (Z_p -> Albedo, Z_s -> Normal_phys)Ôºå
    #      ËÄå‰∏çË¢´‰∏ãÊ∏∏‰ªªÂä°ÁöÑÊ¢ØÂ∫¶Âπ≤Êâ∞„ÄÇ
    if stage == 0:
        # 1. ÂÜªÁªì‰∏ãÊ∏∏‰ªªÂä°Â§¥ (Âè™Êé•Êî∂ Z_s)
        _set_requires_grad(model.head_seg, False)
        _set_requires_grad(model.head_depth, False)
        _set_requires_grad(model.head_normal, False)

        # 2. Á°Æ‰øùÂàÜËß£Â§¥ÂíåÊäïÂΩ±Â±ÇÊòØËß£ÂÜªÁöÑ
        _set_requires_grad(model.projector_s, True)
        _set_requires_grad(model.projector_p, True)
        _set_requires_grad(model.albedo_head, True)
        _set_requires_grad(model.normal_head, True)
        _set_requires_grad(model.light_head, True)
        _set_requires_grad(model.decoder_app, True)

        logging.info("‚ùÑÔ∏è Stage-0: Decomposition Warmup. Task Heads FROZEN. Training Recon/Decomp only.")

    # === Stage 1 & 2: ËÅîÂêàËÆ≠ÁªÉ (Joint Training) ===
    # Ëß£ÂÜªÊâÄÊúâÈÉ®ÂàÜÔºåÂºÄÂßãËÆ≠ÁªÉ‰∏ãÊ∏∏‰ªªÂä°
    else:
        _set_requires_grad(model.head_seg, True)
        _set_requires_grad(model.head_depth, True)
        _set_requires_grad(model.head_normal, True)

        # Á°Æ‰øùÂÖ∂‰ªñÈÉ®ÂàÜ‰πüÊòØËß£ÂÜªÁöÑ
        _set_requires_grad(model.projector_s, True)
        _set_requires_grad(model.projector_p, True)

        logging.info(f"üî• Stage-{stage}: Full Joint Training. All components UNFROZEN.")


def _get_lr(optimizer):
    for pg in optimizer.param_groups:
        return pg.get("lr", None)


def _set_lr(optimizer, lr):
    for pg in optimizer.param_groups:
        pg["lr"] = lr


def _build_scheduler(optimizer, train_cfg):
    """
    Ëá™Âä®ÊûÑÂª∫Ë∞ÉÂ∫¶Âô®ÔºöCosine Êàñ Step
    """
    base_lr = float(train_cfg.get("learning_rate", 1e-4))
    sched_cfg = train_cfg.get("lr_scheduler", {}) or {}
    sched_type = str(sched_cfg.get("type", "cosine")).lower()

    if sched_type == "cosine":
        warmup_epochs = int(sched_cfg.get("warmup_epochs", 3))
        min_lr_factor = float(sched_cfg.get("min_lr_factor", 0.1))
        total_epochs = int(train_cfg.get("epochs", 30))
        t_max = int(sched_cfg.get("T_max", max(1, total_epochs - warmup_epochs)))
        cosine = CosineAnnealingLR(
            optimizer,
            T_max=t_max,
            eta_min=base_lr * min_lr_factor
        )
        return {
            "type": "cosine",
            "warmup_epochs": warmup_epochs,
            "base_lr": base_lr,
            "cosine": cosine
        }

    # fallback: StepLR
    step_size = int(sched_cfg.get("step_size", 100))
    gamma = float(sched_cfg.get("gamma", 0.5))
    step = StepLR(optimizer, step_size=step_size, gamma=gamma)
    return {
        "type": "step",
        "step": step
    }


# ----------------------------
# train loops
# ----------------------------
def train_one_epoch(model, train_loader, optimizer, criterion, device, epoch, stage: int):
    model.train()
    total_train_loss = 0.0
    pbar = tqdm(train_loader, desc=f"Epoch {epoch + 1} [Training]", leave=False)

    # Á¥ØÁßØÊ¢ØÂ∫¶Ê≠•Êï∞
    target_bs = 16
    physical_bs = train_loader.batch_size
    accumulation_steps = max(1, target_bs // physical_bs)

    optimizer.zero_grad(set_to_none=True)

    for i, batch in enumerate(pbar):
        batch = {k: (v.to(device, non_blocking=True) if torch.is_tensor(v) else v) for k, v in batch.items()}
        rgb = batch['rgb']

        # ÂâçÂêë‰º†Êí≠ (FP32)
        outputs = model(rgb, stage=stage)

        crit_out = criterion(outputs, batch)

        # Ëß£Êûê Loss ËøîÂõûÂÄº
        if isinstance(crit_out, (tuple, list)):
            total_loss, loss_dict = crit_out[0], crit_out[1]
        elif isinstance(crit_out, dict):
            loss_dict = crit_out
            total_loss = loss_dict.get('total_loss')
        else:
            raise ValueError("criterion must return dict or (total_loss, dict).")

        # ÊâìÂç∞ÊúÄÂêé‰∏Ä‰∏™ batch ÁöÑ loss
        if i == len(pbar) - 1:
            # ÁÆÄÂåñ logÔºåÈÅøÂÖçÂà∑Â±è
            simple_log = {k: float(f"{v:.4f}") for k, v in loss_dict.items() if
                          isinstance(v, (float, torch.Tensor)) and v > 0.001}
            logging.info(f"Epoch {epoch + 1} Loss: {simple_log}")

        loss_normalized = total_loss / accumulation_steps
        loss_normalized.backward()

        # Ê¢ØÂ∫¶Êõ¥Êñ∞
        if (i + 1) % accumulation_steps == 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)
            optimizer.step()
            optimizer.zero_grad(set_to_none=True)

        # ËÆ∞ÂΩï Loss
        loss_val = total_loss.item()
        if not np.isfinite(loss_val):
            logging.info(f"‚ö†Ô∏è Warning: Non-finite loss {loss_val} at step {i}")

        total_train_loss += float(loss_val)
        pbar.set_postfix(loss=f"{loss_val:.4f}")

    # Â§ÑÁêÜÂâ©‰ΩôÊ¢ØÂ∫¶
    if len(train_loader) % accumulation_steps != 0:
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)
        optimizer.step()
        optimizer.zero_grad(set_to_none=True)

    avg_train_loss = total_train_loss / max(1, len(train_loader))
    logging.info(f"Epoch {epoch + 1} - Avg Loss: {avg_train_loss:.4f}")
    return avg_train_loss


def calculate_improvement(base_metrics, current_metrics, data_type='nyuv2'):
    """Áõ∏ÂØπÊèêÂçáÁéáËÆ°ÁÆó"""
    improvement = 0
    count = 0
    # 1=Ë∂äÂ§ßË∂äÂ•Ω, 0=Ë∂äÂ∞èË∂äÂ•Ω
    metric_meta = {
        'seg_miou': 1, 'seg_pixel_acc': 1,
        'depth_abs_err': 0, 'depth_rel_err': 0,
        'normal_mean_angle': 0, 'normal_acc_30': 1,
        'normal_median_angle': 0, 'normal_acc_11': 1, 'normal_acc_22': 1
    }

    if 'gta5' in data_type:
        valid_keys = {'seg_miou', 'seg_pixel_acc'}
    elif data_type == 'cityscapes':
        valid_keys = {'seg_miou', 'seg_pixel_acc', 'depth_abs_err', 'depth_rel_err'}
    else:
        valid_keys = set(metric_meta.keys())

    for k, direction in metric_meta.items():
        if k not in valid_keys: continue
        if k in base_metrics and k in current_metrics:
            base = base_metrics[k]
            curr = current_metrics[k]
            if base == 0: continue

            if direction == 1:
                imp = (curr - base) / base
            else:
                imp = (base - curr) / base
            improvement += imp
            count += 1

    return improvement / max(1, count)


def train(model, train_loader, val_loader, optimizer, criterion, scheduler, config, device,
          checkpoint_dir='checkpoints'):
    data_type = config['data'].get('type', 'nyuv2').lower()
    train_cfg = config['training']

    stage0_epochs = int(train_cfg.get('stage0_epochs', 0))
    stage1_epochs = int(train_cfg.get('stage1_epochs', 0))
    total_epochs = int(train_cfg.get('epochs', 30))
    base_lr = float(train_cfg.get("learning_rate", 1e-4))

    ind_warmup_epochs = int(train_cfg.get('ind_warmup_epochs', 0))
    target_ind_lambda = float(config['losses'].get('lambda_independence', 0.0))

    best_relative_score = -float('inf')
    baseline_metrics = None
    best_epoch = 0
    best_metrics_details = {}

    sched = _build_scheduler(optimizer, train_cfg)
    logging.info(f"[LR Scheduler] {sched['type']}; base_lr={base_lr}")

    for epoch in range(total_epochs):
        # --- Stage Logic ---
        if epoch < stage0_epochs:
            stage = 0
        elif epoch < stage1_epochs + stage0_epochs:  # Â¶ÇÊûúÊúâstage1ÁöÑËØù
            stage = 1
        else:
            stage = 2

        # ÊØè‰∏™ Epoch Ê£ÄÊü•ÊòØÂê¶ÈúÄË¶ÅÂàáÊç¢ÂÜªÁªìÁä∂ÊÄÅ
        _switch_stage_freeze(model, stage)

        if epoch == 0:
            logging.info(f"üîç [Training Strategy] Total Epochs: {total_epochs}")
            logging.info(f"   Stage 0 (Decomp Only): 0 -> {stage0_epochs}")
            logging.info(f"   Stage 2 (Joint Train): {stage0_epochs} -> {total_epochs}")

        # --- Lambda Decay/Warmup Strategy ---
        current_ind_lambda = target_ind_lambda

        # Âú® Stage 0 Âº∫Âà∂ lambda ‰∏∫ 0ÔºåÂõ†‰∏∫ÂàÜËß£Èò∂ÊÆµÂè™ÈúÄË¶ÅÁâ©ÁêÜLoss
        if stage == 0:
            current_ind_lambda = 0.0
        # Â¶ÇÊûúËøòÂú® Warmup ÊúüÈó¥
        elif ind_warmup_epochs > 0 and (epoch - stage0_epochs) < ind_warmup_epochs:
            progress = epoch - stage0_epochs
            ratio = min(1.0, max(0.0, progress / float(ind_warmup_epochs)))
            current_ind_lambda = target_ind_lambda * ratio
            logging.info(f"   > Ind Loss Warmup: {progress}/{ind_warmup_epochs} = {ratio:.2f}")

        # Êõ¥Êñ∞ Criterion ‰∏≠ÁöÑÊùÉÈáç
        real_criterion = criterion.module if hasattr(criterion, 'module') else criterion
        if hasattr(real_criterion, 'weights'):
            real_criterion.weights['lambda_independence'] = torch.tensor(current_ind_lambda, device=device)

        # ---- LR Warmup (Cosine only) ----
        if sched["type"] == "cosine":
            warmup_epochs = sched["warmup_epochs"]
            if epoch < warmup_epochs:
                warmup_start = 0.1 * base_lr
                ratio = float(epoch + 1) / float(max(1, warmup_epochs))
                lr_now = warmup_start + (base_lr - warmup_start) * ratio
                _set_lr(optimizer, lr_now)

        cur_lr = _get_lr(optimizer)
        logging.info(f"\n----- Epoch {epoch + 1}/{total_epochs} (Stage {stage}) | lr={cur_lr:.6f} -----")

        # --- Train & Validate ---
        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, epoch, stage=stage)
        val_metrics = evaluate(model, val_loader, criterion, device, stage=stage, data_type=data_type)

        # --- Scheduler Step ---
        if sched["type"] == "cosine":
            if epoch >= sched["warmup_epochs"]:
                sched["cosine"].step()
        else:
            sched["step"].step()

        # --- Best Model Selection ---
        # Stage 0 ‰∏çÂèÇ‰∏é Score ÊØîËæÉ
        if stage == 0:
            logging.info("  -> Stage 0 (Decomp) - Skipping improvement calculation.")
            baseline_metrics = val_metrics  # ‰∏çÊñ≠Êõ¥Êñ∞ baseline Áõ¥Âà∞ Stage 0 ÁªìÊùü
        else:
            if baseline_metrics is None:
                baseline_metrics = val_metrics

            score = calculate_improvement(baseline_metrics, val_metrics, data_type=data_type)
            is_best = (score > best_relative_score)

            if is_best:
                best_relative_score = score
                best_epoch = epoch + 1
                best_metrics_details = val_metrics.copy()
                logging.info(f"  -> üèÜ Best Model! Score: {score:.2%}")

            save_checkpoint({
                'epoch': epoch + 1,
                'state_dict': model.state_dict(),
                'optimizer': optimizer.state_dict(),
                'best_score': best_relative_score,
            }, is_best, checkpoint_dir=checkpoint_dir)

    logging.info(f"\n‚úÖ Training Finished. Best Epoch: {best_epoch}, Score: {best_relative_score:.2%}")