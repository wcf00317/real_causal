import yaml, json
import torch
import torch.optim as optim
from torch.utils.data import DataLoader
import os, argparse
import logging
from datetime import datetime

# --- åŸºç¡€å·¥å…· (å¤ç”¨ç°æœ‰) ---
from utils.general_utils import set_seed, setup_logging

# --- [æ–°] CelebA ä¸“ç”¨æ¨¡å—å¯¼å…¥ (è®¡åˆ’åœ¨åç»­æ­¥éª¤ç”Ÿæˆ) ---
# æ³¨æ„ï¼šè¿™äº›æ–‡ä»¶ç›®å‰è¿˜ä¸å­˜åœ¨ï¼Œæ˜¯æˆ‘ä»¬åœ¨"å¹³è¡Œå®‡å®™"ä¸­å³å°†åˆ›å»ºçš„
try:
    from data_utils.celeba_dataset import CelebADataset
    from models.causal_celeba_model import CausalCelebAModel
    from losses.celeba_loss import CelebALoss
    from engine.trainer_cls import train_cls
except ImportError:
    print("âš ï¸ è­¦å‘Š: CelebA ä¸“ç”¨æ¨¡å—å°šæœªå®Œå…¨ç”Ÿæˆã€‚è¯·æŒ‰ç…§è®¡åˆ’ç”Ÿæˆåç»­æ–‡ä»¶ã€‚")


def main(config_path):
    """
    CelebA å› æœè§£è€¦å®éªŒå…¥å£ (40 Attributes + Reconstruction)
    """
    # 1. åŠ è½½é…ç½®å¹¶è®¾ç½®éšæœºç§å­
    try:
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        set_seed(config['training']['seed'])
    except Exception as e:
        print(f"âŒ Error loading config file: {e}")
        return

    # è®¾ç½®æ—¥å¿—ç›®å½•
    timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M')
    run_dir = os.path.join('runs_celeba', timestamp)  # åŒºåˆ†äºåŸæ¥çš„ runs
    checkpoint_dir = os.path.join(run_dir, 'checkpoints')
    vis_dir = os.path.join(run_dir, 'visualizations')

    os.makedirs(checkpoint_dir, exist_ok=True)
    os.makedirs(vis_dir, exist_ok=True)

    setup_logging(run_dir)
    logging.info(f"ğŸš€ CelebA Causal Experiment Started: {timestamp}")
    logging.info(f"ğŸ“‚ Output Directory: {run_dir}")
    logging.info("=" * 60)
    logging.info(json.dumps(config, indent=4, default=str))
    logging.info("=" * 60)

    # 2. è®¾ç½®è®¡ç®—è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logging.info(f"âš™ï¸ Using device: {device}")

    # 3. åˆå§‹åŒ–æ•°æ®é›† (CelebA)
    logging.info("\nğŸ“š Initializing CelebA Dataset...")
    data_cfg = config['data']
    target_num_attr = data_cfg.get('num_attributes')

    # è®­ç»ƒé›†
    train_dataset = CelebADataset(
        root_dir=data_cfg['dataset_path'],
        split='train',
        img_size=data_cfg.get('img_size', [128, 128]),
        num_attributes=target_num_attr,
        augmentation=True
    )

    # éªŒè¯é›†
    val_dataset = CelebADataset(
        root_dir=data_cfg['dataset_path'],
        split='val',  # æˆ– 'test'
        img_size=data_cfg.get('img_size', [128, 128]),
        num_attributes=target_num_attr,
        augmentation=False
    )

    logging.info(f"   Train samples: {len(train_dataset)}")
    logging.info(f"   Val   samples: {len(val_dataset)}")

    train_loader = DataLoader(
        train_dataset,
        batch_size=data_cfg['batch_size'],
        shuffle=True,
        num_workers=data_cfg['num_workers'],
        pin_memory=True,
        drop_last=True
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=data_cfg['batch_size'],
        shuffle=False,
        num_workers=data_cfg['num_workers'],
        pin_memory=True
    )

    # 4. åˆå§‹åŒ–æ¨¡å‹ (CausalCelebAModel)
    logging.info("\nğŸ§  Initializing Model (ResNet18-Based Causal)...")
    model = CausalCelebAModel(config['model']).to(device)

    # 5. ä¼˜åŒ–å™¨ & Loss
    train_cfg = config['training']
    base_lr = float(train_cfg['learning_rate'])

    optimizer = optim.AdamW(
        model.parameters(),
        lr=base_lr,
        weight_decay=float(train_cfg.get('weight_decay', 1e-4))
    )

    # ä¸“ç”¨ Loss æ¨¡å— (BCE + Recon + CKA)
    criterion = CelebALoss(config['losses'], device=device)

    # 6. å¯åŠ¨è®­ç»ƒ (è°ƒç”¨æ–°çš„ trainer_cls)
    logging.info("\nğŸ”¥ Starting Training Loop...")
    if train_cfg.get('enable_training', True):
        train_cls(
            model=model,
            train_loader=train_loader,
            val_loader=val_loader,
            optimizer=optimizer,
            criterion=criterion,
            config=config,
            device=device,
            checkpoint_dir=checkpoint_dir,
            vis_dir=vis_dir  # ä¼ å…¥å¯è§†åŒ–ç›®å½•ï¼Œæ–¹ä¾¿è®­ç»ƒä¸­é€”çœ‹å›¾
        )
    else:
        logging.info("ğŸ›‘ Training disabled in config.")

    logging.info("\nâœ… All Done.")


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    # é»˜è®¤æŒ‡å‘æ–°çš„é…ç½®æ–‡ä»¶
    parser.add_argument('--config', type=str, default='configs/celeba/resnet18_40attr.yaml')
    args = parser.parse_args()
    main(args.config)